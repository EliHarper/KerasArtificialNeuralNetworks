{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Setup:\n",
    "\n",
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"deep\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# SELU activation function example:\n",
    "\n",
    "from scipy.special import erfc\n",
    "\n",
    "# Alpha and scale to self normalize with mean 0 and std dev of 1\n",
    "alpha_0_1 = -np.sqrt(2/np.pi) / (erfc(1/np.sqrt(2))* np.exp(1/2) - 1)\n",
    "scale_0_1 = (1 - erfc(1 / np.sqrt(2)) * np.sqrt(np.e)) * np.sqrt(2 * np.pi) * (2 * erfc(np.sqrt(2)) *np.e**2 + np.pi*erfc(1/np.sqrt(2)) **2*np.e - 2*(2+np.pi)*erfc(1/np.sqrt(2))*np.sqrt(np.e)+ np.pi+2)**(-1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def elu(z, alpha=1):\n",
    "    return np.where(z < 0, alpha * (np.exp(z) - 1), z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def selu(z, scale=scale_0_1, alpha = alpha_0_1):\n",
    "    return scale * elu(z, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: mean -0.00, std deviation 1.00\n",
      "Layer 10: mean 0.00, std deviation 0.97\n",
      "Layer 20: mean -0.01, std deviation 0.96\n",
      "Layer 30: mean 0.01, std deviation 0.97\n",
      "Layer 40: mean -0.03, std deviation 0.94\n",
      "Layer 50: mean 0.04, std deviation 0.98\n",
      "Layer 60: mean 0.05, std deviation 0.96\n",
      "Layer 70: mean 0.00, std deviation 0.90\n",
      "Layer 80: mean -0.04, std deviation 0.90\n",
      "Layer 90: mean -0.01, std deviation 0.90\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "Z = np.random.normal(size=(500, 100)) # standardized inputs (normal)\n",
    "\n",
    "for layer in range(100):\n",
    "    W = np.random.normal(size=(100, 100), scale = np.sqrt(1/100)) # LeCun initialization (shouldn't there be a method for that?)\n",
    "    Z = selu(np.dot(Z, W))\n",
    "    means = np.mean(Z, axis = 0).mean()\n",
    "    stds = np.std(Z, axis = 0).mean()\n",
    "    if layer % 10 == 0:\n",
    "        print(\"Layer {}: mean {:.2f}, std deviation {:.2f}\".format(layer, means, stds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x7fa4f0a4f7b8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using SELU:\n",
    "\n",
    "keras.layers.Dense(10, activation=\"selu\",\n",
    "                  kernel_initializer=\"lecun_normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28,28]))\n",
    "model.add(keras.layers.Dense(300, activation=\"selu\",\n",
    "                            kernel_initializer=\"lecun_normal\"))\n",
    "\n",
    "for layer in range(99):\n",
    "    model.add(keras.layers.Dense(100, activation=\"selu\",\n",
    "                                kernel_initializer=\"lecun_normal\"))\n",
    "\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Now let's trian the model, remembering to scale the inputs to a mean of 0 and std dev of 1:\n",
    "\n",
    "pixel_means = X_train.mean(axis=0, keepdims=True)\n",
    "pixel_stds = X_train.std(axis=0, keepdims=True)\n",
    "X_train_scaled = (X_train - pixel_means) / pixel_stds\n",
    "X_valid_scaled = (X_valid - pixel_means) / pixel_stds\n",
    "X_test_scaled = (X_test - pixel_means) / pixel_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "55000/55000 [==============================] - 54s 976us/sample - loss: 1.1867 - accuracy: 0.5625 - val_loss: 0.7696 - val_accuracy: 0.7252\n",
      "Epoch 2/5\n",
      "55000/55000 [==============================] - 42s 766us/sample - loss: 0.6680 - accuracy: 0.7637 - val_loss: 0.6368 - val_accuracy: 0.7628\n",
      "Epoch 3/5\n",
      "55000/55000 [==============================] - 41s 753us/sample - loss: 0.5605 - accuracy: 0.8034 - val_loss: 0.5448 - val_accuracy: 0.8136\n",
      "Epoch 4/5\n",
      "55000/55000 [==============================] - 44s 791us/sample - loss: 0.5115 - accuracy: 0.8241 - val_loss: 0.4939 - val_accuracy: 0.8280\n",
      "Epoch 5/5\n",
      "55000/55000 [==============================] - 43s 783us/sample - loss: 0.4760 - accuracy: 0.8363 - val_loss: 0.4912 - val_accuracy: 0.8338\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_scaled, y_train, epochs=5,\n",
    "                   validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ^ This shows a model that's suffering from vanishing/exploding gradients.\n",
    "\n",
    "# Batch normalization:\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense_203 (Dense)            (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_204 (Dense)            (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_205 (Dense)            (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 271,346\n",
      "Trainable params: 268,978\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('batch_normalization/gamma:0', True),\n",
       " ('batch_normalization/beta:0', True),\n",
       " ('batch_normalization/moving_mean:0', False),\n",
       " ('batch_normalization/moving_variance:0', False)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn1 = model.layers[1]\n",
    "[(var.name, var.trainable) for var in bn1.variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Operation 'cond/Identity' type=Identity>,\n",
       " <tf.Operation 'cond_1/Identity' type=Identity>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn1.updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "             optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "             metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "55000/55000 [==============================] - 10s 181us/sample - loss: 0.8624 - accuracy: 0.7074 - val_loss: 0.5546 - val_accuracy: 0.8080\n",
      "Epoch 2/10\n",
      "55000/55000 [==============================] - 10s 174us/sample - loss: 0.5752 - accuracy: 0.7997 - val_loss: 0.4771 - val_accuracy: 0.8326\n",
      "Epoch 3/10\n",
      "55000/55000 [==============================] - 9s 165us/sample - loss: 0.5177 - accuracy: 0.8165 - val_loss: 0.4422 - val_accuracy: 0.8414\n",
      "Epoch 4/10\n",
      "55000/55000 [==============================] - 9s 159us/sample - loss: 0.4827 - accuracy: 0.8303 - val_loss: 0.4223 - val_accuracy: 0.8496\n",
      "Epoch 5/10\n",
      "55000/55000 [==============================] - 9s 158us/sample - loss: 0.4604 - accuracy: 0.8375 - val_loss: 0.4069 - val_accuracy: 0.8554\n",
      "Epoch 6/10\n",
      "55000/55000 [==============================] - 9s 158us/sample - loss: 0.4389 - accuracy: 0.8429 - val_loss: 0.3938 - val_accuracy: 0.8578\n",
      "Epoch 7/10\n",
      "55000/55000 [==============================] - 9s 161us/sample - loss: 0.4261 - accuracy: 0.8494 - val_loss: 0.3851 - val_accuracy: 0.8622\n",
      "Epoch 8/10\n",
      "55000/55000 [==============================] - 9s 158us/sample - loss: 0.4145 - accuracy: 0.8523 - val_loss: 0.3786 - val_accuracy: 0.8630\n",
      "Epoch 9/10\n",
      "55000/55000 [==============================] - 9s 169us/sample - loss: 0.4024 - accuracy: 0.8571 - val_loss: 0.3746 - val_accuracy: 0.8646\n",
      "Epoch 10/10\n",
      "55000/55000 [==============================] - 9s 169us/sample - loss: 0.3963 - accuracy: 0.8591 - val_loss: 0.3674 - val_accuracy: 0.8678\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                   validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# In some cases, it's better to apply batch normalization before the activation function:\n",
    "#   Notice the addition of an Activation layer after BN and the 'use_bias=False' swapped in the Dense hidden layers (BN has bias it adds so doing it twice is redundant-ish):\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"relu\"),\n",
    "    keras.layers.Dense(100, use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "             optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "             metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "55000/55000 [==============================] - 10s 175us/sample - loss: 1.0651 - accuracy: 0.6690 - val_loss: 0.6814 - val_accuracy: 0.7914\n",
      "Epoch 2/10\n",
      "55000/55000 [==============================] - 9s 172us/sample - loss: 0.6940 - accuracy: 0.7778 - val_loss: 0.5620 - val_accuracy: 0.8206\n",
      "Epoch 3/10\n",
      "55000/55000 [==============================] - 9s 168us/sample - loss: 0.6022 - accuracy: 0.8010 - val_loss: 0.5021 - val_accuracy: 0.8378\n",
      "Epoch 4/10\n",
      "55000/55000 [==============================] - 9s 161us/sample - loss: 0.5508 - accuracy: 0.8153 - val_loss: 0.4687 - val_accuracy: 0.8456\n",
      "Epoch 5/10\n",
      "55000/55000 [==============================] - 9s 158us/sample - loss: 0.5186 - accuracy: 0.8239 - val_loss: 0.4440 - val_accuracy: 0.8518\n",
      "Epoch 6/10\n",
      "55000/55000 [==============================] - 9s 164us/sample - loss: 0.4948 - accuracy: 0.8313 - val_loss: 0.4264 - val_accuracy: 0.8560\n",
      "Epoch 7/10\n",
      "55000/55000 [==============================] - 9s 165us/sample - loss: 0.4745 - accuracy: 0.8384 - val_loss: 0.4135 - val_accuracy: 0.8594\n",
      "Epoch 8/10\n",
      "55000/55000 [==============================] - 9s 165us/sample - loss: 0.4584 - accuracy: 0.8425 - val_loss: 0.4039 - val_accuracy: 0.8612\n",
      "Epoch 9/10\n",
      "55000/55000 [==============================] - 10s 181us/sample - loss: 0.4444 - accuracy: 0.8462 - val_loss: 0.3927 - val_accuracy: 0.8626\n",
      "Epoch 10/10\n",
      "55000/55000 [==============================] - 10s 183us/sample - loss: 0.4342 - accuracy: 0.8507 - val_loss: 0.3855 - val_accuracy: 0.8638\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                   validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/25\n",
      "55000/55000 [==============================] - 7s 123us/sample - loss: 0.6095 - accuracy: 0.8021 - val_loss: 0.5357 - val_accuracy: 0.8206\n",
      "Epoch 2/25\n",
      "55000/55000 [==============================] - 6s 110us/sample - loss: 0.5148 - accuracy: 0.8369 - val_loss: 0.5767 - val_accuracy: 0.8284\n",
      "Epoch 3/25\n",
      "55000/55000 [==============================] - 6s 111us/sample - loss: 0.5219 - accuracy: 0.8420 - val_loss: 0.5697 - val_accuracy: 0.8416\n",
      "Epoch 4/25\n",
      "55000/55000 [==============================] - 7s 121us/sample - loss: 0.5265 - accuracy: 0.8446 - val_loss: 0.5637 - val_accuracy: 0.8384\n",
      "Epoch 5/25\n",
      "55000/55000 [==============================] - 6s 113us/sample - loss: 0.5371 - accuracy: 0.8471 - val_loss: 0.8142 - val_accuracy: 0.8372\n",
      "Epoch 6/25\n",
      "55000/55000 [==============================] - 6s 110us/sample - loss: 0.5128 - accuracy: 0.8543 - val_loss: 0.6704 - val_accuracy: 0.8330\n",
      "Epoch 7/25\n",
      "55000/55000 [==============================] - 6s 109us/sample - loss: 0.3066 - accuracy: 0.8913 - val_loss: 0.3716 - val_accuracy: 0.8844\n",
      "Epoch 8/25\n",
      "55000/55000 [==============================] - 6s 109us/sample - loss: 0.2541 - accuracy: 0.9075 - val_loss: 0.3840 - val_accuracy: 0.8862\n",
      "Epoch 9/25\n",
      "55000/55000 [==============================] - 6s 110us/sample - loss: 0.2400 - accuracy: 0.9108 - val_loss: 0.4265 - val_accuracy: 0.8742\n",
      "Epoch 10/25\n",
      "55000/55000 [==============================] - 6s 110us/sample - loss: 0.2208 - accuracy: 0.9176 - val_loss: 0.3981 - val_accuracy: 0.8762\n",
      "Epoch 11/25\n",
      "55000/55000 [==============================] - 6s 109us/sample - loss: 0.2137 - accuracy: 0.9206 - val_loss: 0.4540 - val_accuracy: 0.8822\n",
      "Epoch 12/25\n",
      "55000/55000 [==============================] - 6s 110us/sample - loss: 0.2028 - accuracy: 0.9244 - val_loss: 0.4304 - val_accuracy: 0.8862\n",
      "Epoch 13/25\n",
      "55000/55000 [==============================] - 6s 111us/sample - loss: 0.1431 - accuracy: 0.9440 - val_loss: 0.3973 - val_accuracy: 0.8950\n",
      "Epoch 14/25\n",
      "55000/55000 [==============================] - 7s 121us/sample - loss: 0.1288 - accuracy: 0.9500 - val_loss: 0.4199 - val_accuracy: 0.8930\n",
      "Epoch 15/25\n",
      "55000/55000 [==============================] - 6s 110us/sample - loss: 0.1191 - accuracy: 0.9533 - val_loss: 0.4390 - val_accuracy: 0.8928\n",
      "Epoch 16/25\n",
      "55000/55000 [==============================] - 6s 109us/sample - loss: 0.1119 - accuracy: 0.9565 - val_loss: 0.4384 - val_accuracy: 0.8944\n",
      "Epoch 17/25\n",
      "55000/55000 [==============================] - 6s 108us/sample - loss: 0.1056 - accuracy: 0.9589 - val_loss: 0.4576 - val_accuracy: 0.8926\n",
      "Epoch 18/25\n",
      "55000/55000 [==============================] - 6s 116us/sample - loss: 0.0830 - accuracy: 0.9689 - val_loss: 0.4588 - val_accuracy: 0.8916\n",
      "Epoch 19/25\n",
      "55000/55000 [==============================] - 6s 114us/sample - loss: 0.0760 - accuracy: 0.9715 - val_loss: 0.4672 - val_accuracy: 0.8962\n",
      "Epoch 20/25\n",
      "55000/55000 [==============================] - 6s 109us/sample - loss: 0.0715 - accuracy: 0.9737 - val_loss: 0.4805 - val_accuracy: 0.8942\n",
      "Epoch 21/25\n",
      "55000/55000 [==============================] - 6s 110us/sample - loss: 0.0684 - accuracy: 0.9747 - val_loss: 0.4835 - val_accuracy: 0.8928\n",
      "Epoch 22/25\n",
      "55000/55000 [==============================] - 6s 110us/sample - loss: 0.0643 - accuracy: 0.9770 - val_loss: 0.5015 - val_accuracy: 0.8924\n",
      "Epoch 23/25\n",
      "55000/55000 [==============================] - 6s 112us/sample - loss: 0.0547 - accuracy: 0.9814 - val_loss: 0.5010 - val_accuracy: 0.8920\n",
      "Epoch 24/25\n",
      "55000/55000 [==============================] - 7s 124us/sample - loss: 0.0522 - accuracy: 0.9826 - val_loss: 0.5080 - val_accuracy: 0.8924\n",
      "Epoch 25/25\n",
      "55000/55000 [==============================] - 6s 111us/sample - loss: 0.0503 - accuracy: 0.9832 - val_loss: 0.5136 - val_accuracy: 0.8922\n"
     ]
    }
   ],
   "source": [
    "# Performance Scheduling\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor = 0.5, patience=5)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "optimizer = keras.optimizers.SGD(lr=0.02, momentum=0.9)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "n_epochs = 25\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                   validation_data = (X_valid_scaled, y_valid),\n",
    "                   callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneCycleScheduler(keras.callbacks.Callback):\n",
    "    def __init__(self, iterations, max_rate, start_rate=None,\n",
    "                 last_iterations=None, last_rate=None):\n",
    "        self.iterations = iterations\n",
    "        self.max_rate = max_rate\n",
    "        self.start_rate = start_rate or max_rate / 10\n",
    "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
    "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
    "        self.last_rate = last_rate or self.start_rate / 1000\n",
    "        self.iteration = 0\n",
    "    def _interpolate(self, iter1, iter2, rate1, rate2):\n",
    "        return ((rate2 - rate1) * (self.iteration - iter1)\n",
    "                / (iter2 - iter1) + rate1)\n",
    "    def on_batch_begin(self, batch, logs):\n",
    "        if self.iteration < self.half_iteration:\n",
    "            rate = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate)\n",
    "        elif self.iteration < 2 * self.half_iteration:\n",
    "            rate = self._interpolate(self.half_iteration, 2 * self.half_iteration,\n",
    "                                     self.max_rate, self.start_rate)\n",
    "        else:\n",
    "            rate = self._interpolate(2 * self.half_iteration, self.iterations,\n",
    "                                     self.start_rate, self.last_rate)\n",
    "            rate = max(rate, self.last_rate)\n",
    "        self.iteration += 1\n",
    "        K.set_value(self.model.optimizer.lr, rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/25\n",
      "55000/55000 [==============================] - 3s 51us/sample - loss: 0.0506 - accuracy: 0.9830 - val_loss: 0.5244 - val_accuracy: 0.8904\n",
      "Epoch 2/25\n",
      "55000/55000 [==============================] - 3s 50us/sample - loss: 0.0559 - accuracy: 0.9805 - val_loss: 0.5370 - val_accuracy: 0.8864\n",
      "Epoch 3/25\n",
      "55000/55000 [==============================] - 3s 48us/sample - loss: 0.0623 - accuracy: 0.9772 - val_loss: 0.5663 - val_accuracy: 0.8898\n",
      "Epoch 4/25\n",
      "55000/55000 [==============================] - 3s 48us/sample - loss: 0.0749 - accuracy: 0.9717 - val_loss: 0.5777 - val_accuracy: 0.8866\n",
      "Epoch 5/25\n",
      "55000/55000 [==============================] - 3s 46us/sample - loss: 0.0930 - accuracy: 0.9650 - val_loss: 0.5979 - val_accuracy: 0.8866\n",
      "Epoch 6/25\n",
      "55000/55000 [==============================] - 3s 48us/sample - loss: 0.1027 - accuracy: 0.9613 - val_loss: 0.5885 - val_accuracy: 0.8864\n",
      "Epoch 7/25\n",
      "55000/55000 [==============================] - 3s 49us/sample - loss: 0.1285 - accuracy: 0.9533 - val_loss: 0.5733 - val_accuracy: 0.8854\n",
      "Epoch 8/25\n",
      "55000/55000 [==============================] - 3s 51us/sample - loss: 0.1464 - accuracy: 0.9473 - val_loss: 0.6373 - val_accuracy: 0.8766\n",
      "Epoch 9/25\n",
      "55000/55000 [==============================] - 3s 48us/sample - loss: 0.1592 - accuracy: 0.9441 - val_loss: 0.5911 - val_accuracy: 0.8816\n",
      "Epoch 10/25\n",
      "55000/55000 [==============================] - 3s 53us/sample - loss: 0.1989 - accuracy: 0.9321 - val_loss: 0.6194 - val_accuracy: 0.8698\n",
      "Epoch 11/25\n",
      "55000/55000 [==============================] - 3s 59us/sample - loss: 0.2325 - accuracy: 0.9257 - val_loss: 0.6734 - val_accuracy: 0.8702\n",
      "Epoch 12/25\n",
      "55000/55000 [==============================] - 3s 54us/sample - loss: 0.2418 - accuracy: 0.9237 - val_loss: 0.6331 - val_accuracy: 0.8742\n",
      "Epoch 13/25\n",
      "55000/55000 [==============================] - 3s 51us/sample - loss: 0.1748 - accuracy: 0.9400 - val_loss: 0.5671 - val_accuracy: 0.8840\n",
      "Epoch 14/25\n",
      "55000/55000 [==============================] - 3s 47us/sample - loss: 0.1371 - accuracy: 0.9512 - val_loss: 0.5632 - val_accuracy: 0.8922\n",
      "Epoch 15/25\n",
      "55000/55000 [==============================] - 3s 47us/sample - loss: 0.1141 - accuracy: 0.9580 - val_loss: 0.6544 - val_accuracy: 0.8842\n",
      "Epoch 16/25\n",
      "55000/55000 [==============================] - 3s 46us/sample - loss: 0.0844 - accuracy: 0.9685 - val_loss: 0.6435 - val_accuracy: 0.8906\n",
      "Epoch 17/25\n",
      "55000/55000 [==============================] - 3s 47us/sample - loss: 0.0622 - accuracy: 0.9763 - val_loss: 0.6566 - val_accuracy: 0.8894\n",
      "Epoch 18/25\n",
      "55000/55000 [==============================] - 3s 48us/sample - loss: 0.0450 - accuracy: 0.9833 - val_loss: 0.6950 - val_accuracy: 0.8904\n",
      "Epoch 19/25\n",
      "55000/55000 [==============================] - 3s 46us/sample - loss: 0.0314 - accuracy: 0.9887 - val_loss: 0.7261 - val_accuracy: 0.8948\n",
      "Epoch 20/25\n",
      "55000/55000 [==============================] - 3s 48us/sample - loss: 0.0228 - accuracy: 0.9922 - val_loss: 0.7312 - val_accuracy: 0.8910\n",
      "Epoch 21/25\n",
      "55000/55000 [==============================] - 3s 47us/sample - loss: 0.0169 - accuracy: 0.9947 - val_loss: 0.7446 - val_accuracy: 0.8920\n",
      "Epoch 22/25\n",
      "55000/55000 [==============================] - 3s 46us/sample - loss: 0.0136 - accuracy: 0.9963 - val_loss: 0.7616 - val_accuracy: 0.8914\n",
      "Epoch 23/25\n",
      "55000/55000 [==============================] - 3s 46us/sample - loss: 0.0110 - accuracy: 0.9973 - val_loss: 0.7681 - val_accuracy: 0.8924\n",
      "Epoch 24/25\n",
      "55000/55000 [==============================] - 3s 47us/sample - loss: 0.0099 - accuracy: 0.9979 - val_loss: 0.7729 - val_accuracy: 0.8922\n",
      "Epoch 25/25\n",
      "55000/55000 [==============================] - 3s 46us/sample - loss: 0.0093 - accuracy: 0.9982 - val_loss: 0.7730 - val_accuracy: 0.8932\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "n_epochs = 25\n",
    "# Not sure why we're using floor division here, but I'll go with it:\n",
    "onecycle = OneCycleScheduler(len(X_train) // batch_size * n_epochs, max_rate=0.05)\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs, batch_size = batch_size,\n",
    "                   validation_data=(X_valid_scaled, y_valid),\n",
    "                   callbacks=[onecycle])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The validation loss (0.773) and accuracy discrepencies suggest that the model is fairly significantly overfitting the data.\n",
    "#   I'mma fix this with some better regularization techniques:\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
